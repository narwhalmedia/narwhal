# Cloud‑Native Microservices Media Server on Kubernetes: Architecture and Design Patterns

## 1. Service Decomposition & Domain Modeling

A **domain-driven design (DDD)** approach is used to split the system into bounded contexts aligned with business capabilities. In a media server, key contexts might include **Media Library**, **Indexing**, **Downloading**, **Transcoding**, and **Streaming** services, each with a clear responsibility. For example, a **Media Library** service manages metadata of Series, Episodes, and Movies (core media domain), a **Download** service handles fetching media files (e.g. from external sources or torrents), a **Transcoding** service converts media to streamable formats, and a **Streaming** service serves HLS content to clients. Each bounded context has its own **ubiquitous language** and domain model, preventing leakage between contexts (reinforced by anti-corruption layers when interacting with external/legacy systems).

**Aggregates and Entities:** Within each context, model data as **aggregates** – clusters of domain objects treated as a single unit. For instance, in the media domain a *Series* aggregate could encapsulate child *Episode* entities, and a *Movie* is an aggregate on its own. The *Series* aggregate root (with a global ID like UUID) protects consistency of its episodes and is the only object that external code should reference. All changes to an aggregate go through the root to maintain invariants and consistency within that boundary. Each aggregate is persisted atomically (transaction per aggregate) to satisfy consistency boundaries.

**Domain Events:** When something important happens in one domain, a domain event can be recorded. *“Domain Event indicates that something happened in a domain that you want other parts of the same domain (in-process) to be aware of.”* For example, the Download service might raise an event `DownloadCompleted` when a file finishes downloading. Domain events are handled within the same service (in-process) to trigger side-effects without tight coupling – e.g. upon `EpisodeDownloaded`, the Download service’s handler could initiate checksum validation or notify the Media Library. Domain events help decouple business logic between aggregates (one aggregate’s change can trigger logic on another via an event handler). They can also be used to maintain an **audit log** of changes for debugging or recovery purposes. Notably, all domain event handlers complete within the same transaction scope if needed, using patterns like Unit of Work to commit all changes together.

**Integration Events:** For cross-service communication, **integration events** are published after the local transaction commits. These events represent something a service wants others to know (e.g. *MediaTranscoded*, *EpisodeAdded*). For example, when the Transcoding service finishes processing a file, it publishes a `TranscodingCompleted` event that the Media Library service subscribes to, updating its database to mark the media as stream-ready. These events are sent via a message broker (see section on Event-Driven Architecture) and carry only the necessary data (e.g. media ID, output location). By using events, services remain **loosely coupled** – they communicate changes without direct synchronous calls, improving resilience and enabling eventual consistency.

**Repositories and Data Access:** Each context defines **repository interfaces** for its aggregates, implementing methods like `Save(media)` or `FindByID(id)` in the domain layer. The repository acts as a mediator between the domain and the data mapping layer (often a database), providing an in-memory collection abstraction for retrieving and persisting domain objects. For example, a `SeriesRepository` interface in Go might have methods `GetSeries(seriesID) (*Series, error)` and `UpdateSeries(series *Series) error`. The concrete implementation (in the infrastructure layer) uses a SQL or NoSQL database but the domain logic is abstracted from those details. This aligns with **hexagonal architecture** (ports and adapters): the repository interface is a domain port, and the adapter implements it with a specific database. Repositories often work with **Unit of Work** patterns – e.g. wrapping multiple writes in a single transaction. In Go, a unit-of-work can be as simple as a transaction passed through context: multiple repository calls share the same transaction and commit or rollback together at the end of a service operation.

**Specification Pattern:** To avoid hard-coding query logic in services, complex queries can be encapsulated in **specification** objects. For instance, a “Find episodes aired between dates” criteria can be a specification that the repository knows how to interpret (e.g. turning it into a SQL `WHERE` clause). This keeps query details out of domain services and makes them reusable. While not always necessary, specifications can help keep the domain model pure by externalizing filter logic.

**Anti-Corruption Layer:** When integrating with external APIs or legacy systems (say an external metadata provider for indexing), implement an anti-corruption layer (ACL) between the external model and the internal domain model. For example, an Indexing service calling a third-party TV database would not directly use the JSON responses in its core logic. Instead, an adapter translates external structures (e.g. “show” JSON from an API) into the Indexing domain’s *Series* entity, so the external concepts don’t leak into internal code. This translation layer ensures the internal model remains clean and unaffected by quirks or changes in external systems.

**Code Organization & Architecture:** A **clean architecture** approach is followed, dividing code into layers: Domain, Application (service layer), and Infrastructure. The **domain layer** holds entities, value objects (e.g. a value object for video `Resolution` or `Codec`), aggregates, domain events, and domain services (business logic that doesn’t fit an entity). It has no dependency on frameworks or databases. The **application layer** orchestrates use cases: it contains services or command handlers that execute business use cases by loading aggregates from repositories, invoking domain methods, and publishing events. It may also handle **DTO** translation for inputs/outputs. The **infrastructure layer** contains adapters: e.g. repository implementations using a database (SQL, etc.), message broker publishers, external API clients (through ACL), and so on. This is a classic hexagonal architecture setup, which enforces dependency rule (outer layers depend on inner interfaces). Go packages can be structured by context and layer (for example, a `media` module with subpackages `media/domain`, `media/app`, `media/infra`).

**Dependency Injection in Go:** Go doesn’t have a built-in DI container, but we achieve inversion of control by manually wiring dependencies or using lightweight frameworks. For example, the `main()` function can compose components: initialize database connections, instantiate repository structs, then pass them into service constructors. Tools like **Google Wire** (for compile-time DI code generation) or **Uber’s Fx** (runtime DI container) can help manage complex wiring. These tools inject implementations for interfaces at startup, respecting the clean architecture boundaries. In this project, one could use Wire to generate a provider set that knows how to construct, say, a `TranscodingService` by providing it with a `TranscodingJobRepository` implementation, a message broker client, etc., without manual boilerplate. The result is that each service is configured with the appropriate repository and bus adapter at initialization, but the service logic itself only depends on abstractions, keeping it testable and decoupled.

Finally, we document major design decisions as **Architecture Decision Records (ADRs)** in a `docs/adr` directory. For example, an ADR might record the reasoning for splitting “Media Library” and “Transcoding” into separate services, or the choice of an anti-corruption layer for the external indexer API. This practice helps future maintainers understand the context of decisions.

## 2. gRPC Service Design & Implementation

All inter-service APIs use **gRPC** for high-performance, typed communication. We design **proto files** for each service’s API, organizing them by domain context. A typical approach is to have a Proto package per bounded context (e.g. `media.v1` for the Media Library service, `transcoder.v1` for Transcoding service). This helps with clarity and versioning – we can evolve each API independently, bumping the proto package version (v1, v2, ...) if breaking changes are needed in that service’s contract. Within each .proto, we define RPC services and messages for the operations that service provides. For example, a `MediaService` might have RPCs like `ListSeries`, `GetEpisode`, etc., while a `TranscoderService` could have `StartJob` and `GetJobStatus`. These proto files serve as the single source of truth for both server and client stubs in Go and TypeScript.

**Code Generation:** Using a tool like **Buf** or protoc directly, we generate Go server interfaces/types and TypeScript client stubs from the proto definitions. For Go, we use `protoc-gen-go` and `protoc-gen-go-grpc` which produce typesafe Go interfaces and structs for messages. For the TypeScript frontend, we have a couple options: one is to use **gRPC-Web** with the official `protoc-gen-grpc-web` plugin (producing JS/TS stubs that use an HTTP transport), or use a tool like **ts-proto** which generates idiomatic TypeScript classes for messages and uses gRPC-web or Twirp under the hood. In either case, we integrate the codegen into our build (for example, Buf’s `buf.generate.yaml` can orchestrate Go and TS code generation in one step). We ensure the generated code is version-controlled or reproducible in CI so clients and servers stay in sync. When updating a proto, we follow backward compatibility rules: new fields are added with new field numbers (never reusing or removing existing field numbers), and we prefer adding new RPC methods over changing semantics of existing ones. This way, old clients can still function with newer servers (unknown fields are ignored) and vice versa, which is critical because **“even if you control clients, you can’t always synchronously update client and server”**.

**RPC Patterns:** We leverage gRPC’s support for different streaming patterns as needed:

* **Unary RPCs**: the standard request-response calls (used for most CRUD operations, e.g. *GetMovie* returns movie details).
* **Server-streaming RPCs**: where the server sends a stream of messages back for one request. This is useful for sending large results or continuous updates. For example, a *StreamTranscodingProgress* RPC could let a client subscribe to periodic progress messages for a job. We ensure the client reads until it encounters the end-of-stream (gRPC uses HTTP/2 streaming under the hood, signaled by an EOF when done).
* **Client-streaming RPCs**: where the client streams a sequence of messages to the server and then gets one response. This could be used if implementing an upload service (client sends chunks of a video file). In our HLS scenario we might not need this, but it’s available.
* **Bidirectional streaming**: both client and server can send messages arbitrarily. This could be used for real-time communication, but browsers don’t directly support true bi-di with gRPC-web (only server streaming), so bi-di is mainly for internal service-to-service calls or possibly an admin CLI. One internal use might be a live metrics stream between services or interactive control channels.

We design our RPCs to be **idempotent** where possible, especially if we plan to enable retries. For example, if the *DownloadService* has an RPC `StartDownload(MediaID)`, calling it twice should either create only one download or return the same result (perhaps include a client-provided **request ID** to recognize duplicates). *“It should be safe to retry an RPC without knowing whether it was processed”*, which can be achieved by including a client token or timestamp. If an operation is non-idempotent, we document it clearly or avoid automatic retries for it.

**gRPC Server Implementation (Go):** Each Go service implements the gRPC server interface generated from the proto. We typically wrap our service logic in a layer that translates between the protobuf types and our domain layer. For example, `GetSeries(ctx, req)` on the MediaService server will call an application service method (e.g. `mediaApp.GetSeries(id)`) and then convert the domain model to the proto response message. Validation of inputs is done up front (e.g. check required fields in `req`). We use contexts to propagate cancellations and deadlines – gRPC passes a `context.Context` into each handler. If the client cancels or the deadline is exceeded, that context gets canceled, and our handler should listen for `ctx.Err()` and abort any work if not already finished (to avoid wasted computation).

**Interceptors and Middleware:** gRPC in Go allows **interceptors** to intercept RPC calls for cross-cutting concerns:

* *Unary interceptors* wrap single request-response calls.
* *Stream interceptors* wrap streaming calls.

We utilize a middleware chain (using libraries like `grpc-go`’s interceptor chaining or `grpcmiddleware` from grpc-ecosystem) to add functionality such as:

* **Authentication & Authorization:** An interceptor can check metadata for a valid JWT or auth token on each call, returning an `Unauthenticated` status if missing/invalid.
* **Logging:** A logging interceptor logs each call (method name, caller info, duration, result code). We ensure to log with structured fields and include a correlation ID (see Observability section).
* **Metrics:** We plug in an interceptor that instruments calls for Prometheus – e.g. increment a counter for each RPC and observe durations, partitioned by method name and status code.
* **Tracing:** Using OpenTelemetry, we add an interceptor to start a new trace span for each RPC or propagate an incoming trace context, so that distributed traces include server handling as a span.
* **Error translation:** We might wrap the handler to catch panics or errors and convert them to gRPC status codes. gRPC encourages using its `status.Errorf(codes, "message")` errors. Standard Go errors are mapped to `Unknown` by default if not handled, so we explicitly map known error types (e.g. not found -> `codes.NotFound`). We avoid sending detailed internal errors to clients; instead we return a coded error and log the details internally.

These interceptors help implement policies like rate limiting as well. For example, we could add a token-bucket rate limiter in an interceptor to reject or delay calls beyond a threshold – *“Servers are responsible for enforcing local rate limits”*. This is especially useful for public-facing APIs on the gateway.

**Context Propagation and Deadlines:** Every gRPC call has a context that can carry deadlines and metadata. **We enforce deadlines on all client calls** – clients should set a reasonable timeout on each call instead of waiting indefinitely. The server checks `ctx.Deadline()` and may use it to avoid starting work that likely cannot complete in time. If a service, in turn, calls another downstream gRPC (service-to-service call), it should propagate the context or derive a sub-context with a slightly reduced deadline (to leave time for handling and returning). For example, if an incoming request has 5 seconds remaining, the Media service when calling the Transcoding service could set a deadline of 4.8s to give itself a bit of slack to process the response. This propagation ensures an end-to-end timeout – if the user’s request times out, all sub-operations should promptly abort. We also propagate **metadata** like a trace ID or user ID through context, so that downstream services get those values (the Go gRPC library automatically handles sending metadata in context to the next service when using the generated stubs).

**Graceful Shutdown:** The gRPC servers in each service are managed such that they handle OS signals for shutdown (SIGINT/SIGTERM). We implement graceful shutdown by stopping to accept new requests, and giving in-flight requests some time to finish. In Go, `grpc.Server.GracefulStop()` is used, which stops accepting new connections and waits until ongoing RPCs complete or a timeout hits. Meanwhile, our service components (database connections, etc.) also respond to context cancellation so they do not hang the shutdown. We also integrate with Kubernetes **pod termination hooks**: when Kubernetes stops a pod (e.g., during deployment), there is a grace period. Our app receives a SIGTERM, we initiate the graceful stop, and Kubernetes will refrain from killing the container until either it’s done or the grace period expires. This ensures clients don’t get abruptly dropped connections mid-way through a response.

**Connection Reuse & Client-Side Config:** For calling other services, gRPC clients in Go maintain a persistent HTTP/2 connection (called a Channel) to each target. We **reuse connections** rather than create one per request, because connection setup (HTTP/2 handshake) is expensive. Typically, each microservice has a client pool or a singleton gRPC client per downstream service. For example, the Streaming service might keep a singleton gRPC client for the Media service, reused for all metadata fetch calls. gRPC channels are thread-safe and multiplex calls, so a single connection can handle many simultaneous RPCs. We configure client-side **connection pooling** if needed (e.g., if one channel is not enough for throughput, we can open a few). We also enable **built-in retries** for idempotent calls: gRPC (since gRPC-Go 1.40+) supports automatic retries via service config, but it requires the method to be idempotent and some config in the proto (`retry_policy`). If not using that, we implement retries in client code or via a client interceptor with backoff. For instance, transient errors like `Unavailable` (which might occur if a pod is restarting) can be retried after a short delay. We make sure to **only retry safe operations** and incorporate a unique request ID or an idempotency token to avoid duplication side effects.

**Streaming Considerations:** When implementing server-streaming RPCs (like sending a stream of video segments or progress updates), our server code writes to the `grpc.ServerStream` interface iteratively. We must check context in long streams to stop if the client disconnects (to avoid wasting resources). The gRPC library will cancel the context if the client cancels or disconnects, so a loop sending events breaks when `stream.Context().Err()` is set or when a `stream.Send()` returns an error (often indicating the stream is closed). On the client side (especially for TS/web), reading from a stream is asynchronous – we handle partial messages and also the end-of-stream condition (signaled by EOF). We follow the gRPC best practice: *“return `nil` to indicate successful stream completion, or a gRPC error to terminate with error”*. Any errors in a stream cause the stream to terminate with that status, which the client must handle. We use **bidi streaming** in internal services only where appropriate, keeping in mind flow-control (gRPC handles flow-control under the hood, but we avoid writing faster than the network can send by respecting `Send()` return values).

**Error Handling:** We use gRPC status codes consistently for API errors. For example, if a client requests a media ID that doesn’t exist, we return `NotFound` instead of a generic error. Business rule violations (e.g. trying to download a media that is already in process) might return `FailedPrecondition` or `AlreadyExists` depending on scenario. We avoid encoding errors in response messages; instead we rely on gRPC’s rich error model (with optional details if needed). As one best practice notes, *“Do not include errors in the response payload in most cases – client logic gets complicated… use gRPC status instead”*. On the Go side, this means using `status.Error(codes.Code, "msg")`. Also, we ensure that our interceptors map common Go `context` errors: when a context deadline exceeds or is canceled, gRPC already maps those to `DeadlineExceeded` or `Canceled` for us. If an internal function returns an `sql.ErrNoRows`, we translate it to a `codes.NotFound`. This consistent mapping allows the TypeScript client to handle errors by gRPC code (e.g., show a 404 message vs a 500 for internal errors).

In summary, gRPC gives us a performant, strongly-typed IPC mechanism that, combined with careful design (proto versioning, deadlines, interceptors, and idempotency), yields robust service-to-service communication. All services expose only gRPC endpoints to each other. For external access (like the browser client), we will expose a REST/HTTP gateway as described next.

## 3. Kubernetes-Native Design Patterns

Our system is designed to run on **Kubernetes**, and we embrace Kubernetes-native patterns by extending the Kubernetes API with custom resources for our domain concepts. Key domain constructs like *MediaLibrary*, *TranscodingJob*, and *DownloadQueue* are modeled as **Custom Resource Definitions (CRDs)**. By doing so, we allow Kubernetes itself to track and manage these as first-class objects, and we implement controllers (operators) that reconcile the desired state defined in these resources with the actual cluster state.

**Custom Resource Design:** We define CRDs for the major high-level entities:

* **MediaLibrary CRD:** Represents the state of the media library or a collection of media items. For example, it could contain spec fields like `scanPath` (if pointing to a filesystem or bucket to index) or preferences like default quality, and a status section summarizing number of series, last scan time, etc. More granularly, we might have CRDs for each Series or Movie as well, but those could also remain in a SQL database rather than CRDs. The purpose of a MediaLibrary CRD is mainly to allow Kubernetes to trigger and monitor actions like “index this library location,” etc. If external storage is used, the CRD might just point to it.
* **DownloadJob or DownloadQueue CRD:** We can model pending and active downloads as custom resources. A **DownloadJob** CRD could have spec fields such as `url: "<source URL>"` or torrent magnet, target location, and a status with progress, speed, etc. Alternatively, a single **DownloadQueue** CRD might list multiple items to download (though a one-CRD-per-download is more in spirit of Kubernetes Jobs). We lean toward one CRD per download to leverage controller patterns similarly to K8s Jobs.
* **TranscodingJob CRD:** This is very useful – it represents a request to transcode a specific media file into HLS (or other formats). The spec might include the source video reference (e.g. a PVC path or object storage URL), desired output formats (e.g. resolutions/bitrates or an HLS preset), and perhaps priority. The status would contain current phase (Pending, Running, Succeeded, Failed), progress percentage, output location, error messages, etc.

**Controller Implementation:** For each CRD, we implement a Kubernetes controller (using Kubebuilder or the Operator SDK) that watches those resources and takes action. Each controller runs as a Kubernetes deployment (often combined into a single operator binary if related). As the official pattern states, *“Operators are processes connecting to the master API and watching for events on specific resource types. When an event occurs on a watched resource, a reconcile cycle is started.”* In our case:

* The Transcoding controller watches **TranscodingJob** resources. When a new TranscodingJob CR is created (event: "Added"), the controller’s Reconcile logic will notice the job is in a spec wanting processing but no outcome yet. It will then **spawn a Kubernetes Job** (a built-in K8s Job resource) which runs an FFmpeg pod to perform the transcoding. The controller might set an owner reference so that the Job is tied to the CR for cleanup. It then updates the TranscodingJob status to mark it as Running. It continuously watches for updates – for instance, if the Job pod writes progress to some status (perhaps via heartbeat or by updating the CR status if the pod can access the K8s API). Once the pod finishes, the controller catches the Job completion event and updates the CR status to Succeeded (or Failed with error info). It may also write the output location into the status. If a TranscodingJob CR is deleted by the user, a finalizer can ensure any running pod is terminated before removal.
* The Download controller does similarly for DownloadJob CRs, possibly managing an internal worker or creating a Kubernetes Job that performs the download. (If downloads are long-lived, using K8s Jobs for them is sensible to leverage retry policies and to separate concerns).
* The MediaLibrary controller might watch for changes that require re-indexing, or manage child resources. For example, if a MediaLibrary CR has a path, the controller could create subordinate CRs or Jobs to scan that path and update a database with media info. It could also react to events from Download/Transcode controllers (via CR statuses or via events) to update library state.

Each controller employs the **Informer/Workqueue pattern** under the hood. The controller runtime will queue reconcile requests for resources that changed, and the Reconcile function must be **level-based** (idempotent with respect to the desired state) as per K8s design. This means our reconcile logic, given a TranscodingJob CR, will always drive the system toward the spec. For example, if `spec.sourceVideo=movie1.mkv` and no pod exists, it creates one; if a pod already exists and is running, it might update status; if `spec.cancel=true` (suppose we allowed cancellation via spec), then if a pod is running it will delete it. This level-based approach handles missed events gracefully – even if an event is lost, the next periodic resync or another event will cause reconcile to run and correct any drift.

We carefully design the **CRD schema** (with proper OpenAPI validation, defaulting, etc.) and support **versioning** of CRDs (apiVersion v1alpha1, v1beta1, v1 as we evolve them). We also utilize **Kubernetes conventions** for naming and status conditions. For instance, a TranscodingJob CR status might include standard `conditions` array with `Ready`, `Progressing`, `Failed` conditions and reason messages, so `kubectl` users or UIs see something familiar. We also use **finalizers** if needed – e.g., a `TranscodingJob` finalizer could ensure that if a CR is deleted while a job is running, the controller will clean up the running pod first, preventing orphan work.

**Leader Election & High Availability:** Our operator deployment (the set of controllers) is run with multiple replicas for high availability, but we enable the built-in leader election in controller-manager so that only one instance actively reconciles at a time (avoiding duplicate processing). If the leader dies, another takes over seamlessly. We use the controller-runtime leader election mechanism which uses a ConfigMap or Lease in the cluster to elect a leader. This ensures **HA** without double work – at most one active reconciler per resource kind.

By building these as Kubernetes operators, we gain several benefits:

* Ops teams can use familiar `kubectl` tooling to inspect and manage media jobs (e.g., `kubectl describe transcodingjob/video1` to see its status and events).
* We can leverage Kubernetes features like events and status conditions to inform users. For example, the controller can emit Kubernetes Events on CRs for noteworthy occurrences (transcoding started, 50% done, failed due to error X, etc.), which can be seen via `kubectl get events`.
* It’s cloud-agnostic: we aren’t using a proprietary service for a job queue – instead, Kubernetes itself schedules our work (download and ffmpeg pods).
* We can utilize well-known patterns like **Jobs for work-queue processing** (each TranscodingJob CR essentially triggers a Kubernetes Job which acts like a worker from a queue).

**Example Workflow:** A user (or automated process) creates a `DownloadJob` CR to fetch a new video. The Download controller sees the new CR and launches a downloader pod. When done, it updates the CR status and perhaps creates a new `TranscodingJob` CR for the next step (this could be done via an event as well instead of directly creating CR, to keep bounded contexts separate an event might be cleaner). The Transcoding controller then spins up an FFmpeg job pod to generate HLS streams. As pods complete, statuses are updated and maybe the MediaLibrary controller is informed (via an event or by it watching these CRs too) to add the new media to the index. This string of CRs and controllers effectively forms a *workflow* but with each step managed declaratively in Kubernetes.

We carefully manage **RBAC** for our operators – they need permissions to create/watch pods, jobs, etc., and their custom resources. We define those in the operator manifests.

Using CRDs and controllers adds some complexity, but it aligns with the cloud-native philosophy and makes our system behave like an extension of Kubernetes. If we need to autoscale transcoding workers, we simply let Kubernetes handle it via the Job and HPA (Horizontal Pod Autoscaler) if needed (for example, the Transcoding operator could maintain a pool of worker Deployments, or just rely on jobs which Kubernetes can scale based on queue length – though a custom autoscaler might also be implemented; see Event-Driven Autoscaling in the next section).

## 4. Event-Driven Architecture Patterns

To decouple services and handle asynchronous processing, we employ an **event-driven architecture**. The system uses a **message broker** to publish and subscribe to events between services. We will consider **NATS JetStream vs Apache Kafka** for this role, as they represent two ends of the event streaming spectrum.

**NATS JetStream vs Kafka:** NATS is a lightweight, high-performance messaging system with support for persistence (JetStream) and flexible **pub/sub** semantics. Kafka is a distributed log designed for high throughput and durable event storage. Each has strengths: *Kafka excels at reliably handling high-throughput, persistent streams, while NATS excels at low-latency, lightweight messaging across distributed systems*. NATS has a simpler deployment (a single binary, no external dependencies in its modern form) and uses a subject-based routing that’s very flexible. Kafka has stronger ordering and persistence by default (retaining messages on disk for days), which is useful for event sourcing or replay. Since we have a “no cloud lock-in” constraint, both Kafka and NATS (which are open source) are viable. If we anticipate very large event volumes, need stream processing, or need to replay events from history (for rebuilding state or reprocessing), Kafka might be more suitable. If we want a simpler, more cloud-native (and Kubernetes-friendly) solution, NATS (possibly with JetStream enabled for at-least-once delivery) could be a better fit. Both can provide durability: *“Apache Kafka and NATS JetStream both provide strong durability guarantees, writing messages to disk and supporting replication”*. Kafka provides strict ordering within partitions, whereas NATS streams can also ensure ordering per subject. We lean toward using **NATS JetStream** in this design for its simplicity and flexible topology (subjects allow fine-grained event topics, and the system can easily have many subscribers).

Regardless of broker, we define a **clear schema for events**. We treat events as **first-class messages** possibly defined in protobuf (so we can version and generate code for them too). We could have a dedicated `events.proto` for cross-service events (or use tools like a schema registry if using Avro/JSON). For example, an event could be `MediaDownloaded { mediaId: string; location: string; }`. Using protobuf for events ensures type safety and schema evolution (backward compatible changes like adding new fields).

**Event Types:** We categorize events into:

* **Domain Events:** as discussed, emitted internally within a service boundary to trigger local side effects (these might not leave the service’s process). For instance, Download service raising `DownloadCompleted` to update its own state machine and then publishing an integration event.
* **Integration Events:** events meant for other services. These are often a result of a domain event. E.g., after `DownloadCompleted` (domain event), the Download service publishes an integration event `MediaItemDownloaded` onto NATS/Kafka for whoever is interested (Transcoding service). These are usually **notification events** – telling others something happened (one-way, no immediate response expected).
* **Command Events:** sometimes we also implement command-style messages where one service places a message to request another to perform an action. For example, an Orchestrator service could publish a `TranscodeMediaCommand` event with details, which the Transcoding service listens to. In practice, this might just be a normal event but conceptually it’s asking for something to be done (some systems use a separate channel or a reply mechanism, but often a command message is processed like any other event).
* **Reply/Response Events:** If truly needed, we could implement a request/reply on the bus (NATS supports inbox replies, Kafka generally doesn’t do direct replies easily but one can simulate by correlating messages). However, gRPC already covers synchronous requests between services. So our eventing is primarily async and one-way. We design events such that the emitter doesn’t expect a direct response – instead, it might update status via another mechanism (or simply trust the other service to emit its own event upon completion).

**Distributed Transaction Patterns:** When a business process spans multiple services (distributed transaction), we cannot use traditional ACID transactions. Instead, we rely on the **Saga pattern** to maintain consistency, using either choreography (event-driven) or orchestration:

* In a **choreography saga**, each service involved in a process reacts to events and produces subsequent events. There is no central coordinator; the saga’s flow is embedded in the event interactions. This is highly decoupled – e.g., when a new media is added, MediaLibrary service emits `MediaAdded` event, Download service catches it and starts downloading, then emits `MediaDownloaded`, Transcoding catches that, etc. This approach is **decentralized and initially simple**, but it can be **hard to understand or debug the overall flow**, since *“the order of the overall saga is distributed throughout the code”*. If something goes wrong, it can be challenging to trace which step failed or to coordinate compensations.
* In an **orchestration saga**, a single orchestrator (could be a dedicated service or even a workflow engine like Temporal) sends explicit commands to each step and awaits replies, handling the sequence and any rollbacks centrally. *“One service (a ‘super microservice’) functions as the message broker telling others what to do… orchestration centralizes control flow, making debugging and understanding easier”*. The downside is the orchestrator is a single point of coordination (if it fails, the saga halts – though using a fault-tolerant orchestrator like Temporal mitigates that). In our context, if we find the event chain for adding new media too complex to manage via pure events, we could introduce an Orchestrator service (or use a Temporal workflow) that coordinates: e.g., on user adding media, orchestrator says “DownloadService, download X”, waits, then “TranscodeService, transcode X”, etc., and handles failures by perhaps issuing compensating actions (like if transcode fails, maybe mark the media as unavailable or trigger a cleanup).

Given that our system is fairly pipeline-like (download -> transcode -> serve), a choreography approach can work: each stage just emits an event for the next. We must document the flow clearly since it’s implicit. The orchestrator approach could be considered if we need more complex decision making or if rollback actions are needed on failure of mid-steps. For instance, if download fails, do we simply mark failure (that’s fine). If transcode fails, maybe we want to retry or eventually mark the media as partially available. Those can be handled within that service’s logic and by events (e.g., Transcoding service can emit `TranscodingFailed` event that MediaLibrary catches and marks that media entry as in error state).

We also implement **compensating actions** for certain operations if needed. A typical saga compensation in our domain: if a user deletes a media item, this might need to trigger compensations like cancel any ongoing downloads or transcodes for it and remove files. This can be done via events as well (MediaLibrary publishes `MediaRemoved`, other services listen and act accordingly).

**Event Delivery Semantics:** With NATS JetStream or Kafka, we achieve at-least-once delivery by default (Kafka by design, NATS JetStream with appropriate ack). Consumers should be idempotent because duplicates can occur. For example, if `MediaDownloaded` event is processed twice by Transcoding, it should ideally detect that a transcode job is already scheduled or running and not duplicate it. We include unique IDs in events (like the mediaId and a eventId) to help with idempotency. We might use a small in-memory or Redis cache to de-duplicate if necessary.

**Message Ordering:** With Kafka, if we partition by mediaId, all events for a given media will be in order on the same partition. With NATS, events on the same subject are delivered in order (unless a consumer fails and restart causes replay out of order, but JetStream can maintain sequence). We ensure that ordering requirements are minimal – generally, `MediaDownloaded` will happen after its corresponding `MediaAdded`, etc. Each service can handle out-of-order or missing events gracefully (e.g., if Transcoding receives a `MediaDownloaded` for a media it doesn’t know about yet, maybe the MediaLibrary event hasn’t propagated – the Transcoding service could call Media service via gRPC to fetch metadata, or simply wait until it eventually gets the metadata).

**Event Schema Registry:** If using Kafka and Avro/JSON, we’d likely use Confluent Schema Registry or similar to manage schema versions for events. With protobuf, the `.proto` definitions serve as the schema and we ensure backward compatibility in changes. We might also implement a lightweight **schema registry** ourselves – e.g., storing proto descriptors in a well-known Git repo or service, so that subscribers can validate incoming events against the schema version they expect.

**CQRS (Command Query Responsibility Segregation):** The read side of our system (queries) can be served by materialized views that are kept up-to-date by events. For instance, we might maintain a *search index* for media that is updated whenever media metadata changes (via events) – this index (maybe ElasticSearch or just a denormalized SQL table) would serve queries quickly without hitting all microservices. This is a form of CQRS where write model (normalized in services) and read model (combined view) are separate. In our design, the Media Library service itself might act as the query handler by storing all necessary data. But if we needed a specialized view (like an aggregate popularity or recently added list), we could have a small service subscribe to events and maintain that view.

**Distributed Event Tracing:** We utilize correlation IDs or trace IDs in events as well. For example, if a user action initiated a series of events, we attach a correlation identifier (could be the gRPC context trace ID or a saga ID) in the event metadata. This way, we can trace across services which events and actions were part of the same flow.

In summary, we use events to glue the services asynchronously, favoring eventual consistency and loose coupling. The combination of **NATS JetStream** (for speed with persistence) with clearly defined **integration events** and possibly a lightweight saga orchestration yields a system where services are highly independent. Major decisions like choosing NATS vs Kafka will be recorded in an ADR, considering factors like operational complexity, volume needs, and team expertise. (For instance, an ADR might note that NATS was chosen due to lower latency and simpler ops for our moderate throughput needs, while acknowledging Kafka could be swapped in later if requirements grow.)

## 5. Data Access Layer Architecture

Each microservice manages its own data storage, adhering to the **database-per-service** pattern to avoid tight coupling through a shared database. The **Data Access Layer** is carefully designed for both **performance** and **maintainability**.

**Repository Pattern & Unit of Work:** As mentioned, we employ a Repository pattern where domain objects are retrieved and persisted through repository interfaces. For example, the Media service might have `SeriesRepository`, `EpisodeRepository`, etc., the Transcoding service might have `TranscodingJobRepository`. These repositories hide the details of whether data is in PostgreSQL, MongoDB, etc., and provide intent-based methods (like `FindReadyToStreamEpisodes()` rather than exposing raw queries in service code). Complex querying needs are handled either by repository methods or by passing in Specification objects that encapsulate query criteria.

For coordinating multiple updates, we use a **Unit of Work** (UoW) approach. In Go, this can be implemented by having repository methods accept a context that carries a transaction, or by explicitly beginning a transaction in an application service and passing a transaction-bound repository to multiple functions. One approach is to have a higher-level `Store` interface that can encompass multiple repository actions in a transaction:

```go
type MediaStore interface {
    BeginTx(ctx) (MediaUnitOfWork, error)
}
type MediaUnitOfWork interface {
    SeriesRepo() SeriesRepository
    EpisodeRepo() EpisodeRepository
    Commit() error
    Rollback() error
}
```

This pattern allows, for instance, adding a new Series and Episode together and committing once. Alternatively, simpler: use the SQL `BEGIN; ... COMMIT;` around multiple calls. The important part is that our domain services can treat multiple writes as atomic when needed (though many operations are on a single aggregate anyway, which is naturally one transaction).

**SQL Database Access:** For relational storage, we choose appropriate Go data access technology:

* **database/sql (with SQLX):** Using Go’s standard `database/sql` with the help of **sqlx** is a popular approach. sqlx extends the stdlib by offering conveniences like `Select` into structs, named query parameters, etc., without an heavy ORM layer. It keeps us writing SQL queries, which is fine since we can optimize them as needed. This is quite performant because there’s minimal abstraction overhead – essentially just scanning rows into structs. It’s also flexible: any complex query can be written by hand.
* **GORM:** GORM is a widely used full-featured ORM in Go. It allows defining models as structs and provides an API to build queries in Go rather than writing SQL. GORM brings features like migrations, relationship handling (e.g. preload associations), and a large community. However, it has some runtime overhead and a learning curve. GORM’s dynamic nature (using reflection) can make it less type-safe and sometimes less performant for bulk operations. That said, it is convenient for rapid development. GORM supports all major databases and can even generate schema from structs. It also provides a robust transaction API and even multi-database sharding support. Many developers find that *“after the initial learning curve, GORM allows writing queries with minimal boilerplate and rarely touching raw SQL”*. We might choose GORM for, say, the Media Library service if we want quick CRUDs for a lot of tables.
* **Ent (entgo):** Ent is a newer ORM that uses a **code-generation, schema-as-code** approach. We define the schema in Go code (as node/edge definitions), Ent generates typesafe query builders. It catches many errors at compile time and tends to have better performance than reflection-based ORMs. Ent is great for complex object graphs and ensuring correctness (type-safe). However, ent’s generated code can be quite large and it has a monolithic approach that might be overkill for small microservices. Also, using ent in multiple microservices could mean duplicating some code or maintaining separate schema packages for each service (which is fine). If we anticipate very complex queries and want the benefits of static typing, ent is a strong choice. We note that ent generation can slow compile times a bit and needs careful integration in our build process.

We will evaluate each per service. For example, the Download service might not need a heavy ORM at all – it could simply use a key-value store or a lightweight SQL table for tracking jobs, so `sqlx` might suffice. The Media Library, with relational data (series-episodes), could use GORM or ent to easily manage relationships (e.g., one series to many episodes). An **ADR** will document the choice (e.g., “Use Ent for Media Library for type-safe complex queries, use sqlx for simpler services”). We consider performance too: a Medium post benchmarking ORMs found GORM and Ent both fairly performant for typical operations (differences in the microseconds to milliseconds range), with GORM sometimes surprisingly fast for simple queries, and ent excelling in compile-time safety.

**SQL vs NoSQL:** Likely, the media metadata (Series/Episode/Movie info) fits well in a SQL database (structured, relational). The Download and Transcoding job statuses could also be SQL (with job tables) or a NoSQL like Redis (which might be used for quick transient state). If we use Redis, it could act as a distributed cache (more on caching below) and also store ephemeral job states. But for durability, a SQL or a proper persistent store is safer (so that if our service restarts, we don’t lose track of an ongoing download – though Kubernetes Jobs themselves provide some durability in their status, duplicating that in our DB is also possible).

**Migrations:** Managing schema changes is important from day one. We use a migration tool such as **golang-migrate** or **pressly’s goose**. Golang-migrate is very popular (10k+ stars) and supports many DBs out of the box. It uses versioned `.sql` files (or Go scripts) that we can apply up/down. Goose similarly supports SQL or Go-based migrations and is straightforward. Goose’s advantage is the ability to run arbitrary Go code in a migration (for complex data fixes), whereas golang-migrate focuses on SQL changes. We might use Goose for flexibility; for example, if we need to backfill some data during a migration, a Go migration can load some context and perform it. Both tools integrate with CI/CD – e.g., we can run `goose up` on startup or in a Kubernetes init container to ensure the DB schema is current. Another interesting tool is **Atlas** for a declarative approach, but that might be overkill for now. We will write migrations for creating tables like `series`, `episodes`, `downloads`, etc., and use semantic versioning in the filenames so we know the order. The migration strategy is documented so that deploying a new version of a service runs the migrations safely (with downtime or zero-downtime techniques as needed – e.g., add columns in backward-compatible ways, etc.).

**Caching Strategies:** To improve read performance and reduce database load, we incorporate caching where appropriate. We use **Redis** (as an in-memory data store) for distributed caching and transient data. There are a few caching patterns to apply:

* **Cache-Aside (Lazy Loading):** This is the primary pattern. For instance, when the Streaming service needs to get episode metadata (title, duration, etc.) to populate an HLS manifest, it will first check a Redis cache. If the data is not there (cache miss), it will query the Media Library service (or its database), then populate the cache with the result for next time. We ensure that cache entries have appropriate TTL. This strategy keeps the cache up-to-date on access. *“In cache-aside, the application is responsible for fetching from DB on a miss and populating the cache”*. Writes go directly to the database, and the cache is *invalidated* for that data.
* **Write-Through:** In some cases, when our service writes data, we can put it in the cache at the same time as the database. For example, when the Media service creates a new Episode entry in DB, it could also set that in Redis. However, write-through adds latency to writes and isn’t always needed if reads of new data are infrequent. We often prefer to simply invalidate cache on writes (delete the key or mark stale). According to the Azure caching guide, *the application can follow write-through by updating the DB and invalidating the cache entry, so next read causes reload*. This ensures subsequent readers get fresh data without stale reads.
* **Read-Through:** Some caches (like a caching database proxy or an ORM-level cache) can automatically populate on miss. We aren’t using such an advanced cache aside from maybe an HTTP reverse-proxy cache for content. But we can mimic read-through by providing helper functions that wrap the get-check-store logic.

We take care to handle **cache invalidation** correctly – a famously hard problem. When a media item is updated (say, user edits metadata for an episode), our service will update the DB and then **evict the cache** for that item’s key. If multiple services have caches (e.g., Media service and Streaming service might both cache some data), we might use a **message or shared cache** to coordinate. A simple approach is to use Redis pub/sub or a NATS message to broadcast an invalidation message (e.g., “episode 123 updated”) so other services flush their caches for that item. Alternatively, use a central cache (Redis) that all services share, so the first one updating can delete the key and others see it gone.

For heavy content like HLS playlists, we might use a combination of in-memory caching in the service process (Go map with an LRU) for ultra-fast access plus Redis as a distributed backup. But given that the HLS content itself is static files (which might be cached at CDN or client), caching is more relevant to metadata queries and perhaps authentication tokens, etc.

**Distributed Cache:** We configure Redis (or Memcached, but Redis offers more features like persistence and pub/sub) as a separate deployment in Kubernetes (or use a managed one if allowed). Our Go code uses a client like `go-redis` which is battle-tested. We consider using Redis also for ephemeral data such as pending queue lengths, which the autoscaler might use.

**Data Model Choices:** We use **SQL (PostgreSQL)** for the core media metadata because of its relational nature (and perhaps use JSON columns for some flexible fields like metadata blobs). For high-speed lookups of simple key-value (like session tokens or caching user preferences), Redis is used. Download and Transcode records could live in SQL (for permanence and querying history) but also be cached or duplicated in Redis for quick status checks (since these are short-lived records, sometimes people keep them just in memory or ephemeral store – but we want to survive restarts, so SQL is safer).

**SQL Toolkit:** If using `database/sql` or `sqlx`, we may also consider **sqlc**, which generates type-safe code from SQL queries (like a inverse of ent – you write SQL, it produces Go code for you). sqlc gives the best of both: you control the SQL and get compile-time checks. We mention it for completeness: it could be part of our stack if team prefers raw SQL but wants safety. In any case, the team should follow a consistent style. We will produce a **Code Style Guide** for SQL (to avoid common mistakes like not using parameterized queries). For security, we always use prepared statements or ORM mechanisms to avoid SQL injection (the security best practices highlight this: *“Use parameterized queries or an ORM to avoid SQL injection”*).

**Performance considerations:** We will benchmark critical queries (maybe using EXPLAIN in Postgres to ensure indexes are used for things like looking up episodes by series or searching by name). We’ll add necessary indexes (e.g., index by media ID, by status for quick retrieval of “all pending transcode jobs”). If using ORMs, we ensure N+1 query issues are mitigated (GORM and Ent have ways to preload or eager-load associations so you don’t query in a loop). For high read throughput, caching as mentioned will help. We also leverage the database’s features: e.g., using Redis for caching, but Postgres itself can handle quite a load for moderate traffic if tuned and using connection pooling.

**Transactions and Isolation:** Each service’s database operations are done in transactions where needed. E.g., adding a series and episodes together in one transaction for consistency. PostgreSQL default isolation (Read Committed) is fine for these use cases (we don’t have extreme contention, mainly single-user operations). If a saga updates multiple services, each service transaction is separate but the saga ensures overall consistency via events/compensations.

In summary, our Data Access Layer uses a pragmatic mix of tools:

* Golang libraries (sqlx, GORM, or Ent) chosen per service needs,
* A robust migration process (with goose or migrate),
* A distributed cache (Redis) applying cache-aside and write-through strategies for performance,
* Repository abstraction for testability and adherence to DDD, and
* Sound practices like using transactions and avoiding unsafe queries.

We also integrate **golangci-lint** with SQL/ORM checks (there are linters that catch using raw strings in queries or unparameterized queries). And our CI might run **gosec** which will flag any SQL injection risks (e.g., building SQL by string concatenation). This ensures code quality in data access.

## 6. Transcoding Service Architecture

The **Transcoding service** is responsible for converting raw video files into streaming-friendly formats (HLS in our case). This is a CPU (or GPU) intensive operation that can take significant time per file, so the architecture must handle concurrency, job scheduling, and resource management carefully.

**Work Queues with Kubernetes Jobs:** Instead of managing our own thread pool or job queue inside the service, we offload execution to Kubernetes by using **Job** resources for each transcoding task. When a TranscodingJob custom resource is created (or when an event like `MediaDownloaded` is received, if we trigger jobs event-driven), the Transcoding service’s controller (or even just the service via the Kubernetes API) will create a corresponding Kubernetes `Job` object. This Job spawns a Pod to run the actual FFmpeg process. We package FFmpeg and necessary tooling in a container image (likely based on a lightweight Linux with FFmpeg installed). Each job Pod processes one video and then exits. Kubernetes Jobs are a natural fit as they handle retries (you can set `.spec.backoffLimit`), completions, and failures out-of-the-box. They also integrate with the cluster’s scheduler, so transcoding workloads can be distributed across nodes.

We configure these Jobs with appropriate resource requests: e.g., request 4 CPUs and some memory. According to one real-world tuning, *“allocating 4 CPUs per transcoder job processed a 1h video in \~15-25 minutes (25-40% of runtime) – beyond 4 CPUs, returns diminished”*. We use this insight to allocate about 4 vCPUs to each FFmpeg pod, which is a sweet spot of throughput vs cost. If we had GPU capabilities and needed faster transcodes, we could allocate GPUs to jobs (using Kubernetes device plugins), but an analysis by Egnyte found *“while GPUs made transcodes faster, it wasn’t cost-effective compared to scaling CPU jobs”*. Thus, we stick to CPU jobs, scaling horizontally.

**Adaptive Bitrate (HLS) Pipeline:** We set up FFmpeg commands to generate multiple output variants. Typically, we want to produce HLS master playlist with, say, 1080p, 720p, 480p renditions. FFmpeg can do this in one process by using complex filter graphs or sequentially. We likely use an FFmpeg invocation that encodes multiple resolutions in parallel pipelines to avoid multiple passes (if CPU can handle it). The command might look like:

```
ffmpeg -i input.mp4 \
  -vf scale=w=1920:h=1080:force_original_aspect_ratio=decrease -c:v libx264 -b:v 5M -maxrate 5M -bufsize 10M -c:a aac -ac 2 -ar 48000 -var_stream_map "..., etc." \
  -vf scale=w=1280:h=720:... ... \
  -f hls -hls_time 6 -hls_playlist_type vod \
  -master_pl_name master.m3u8 \
  -hls_segment_filename "v%v/fileSequence%d.ts" \
  -use_localtime_mkdir 1 \
  output_%v.m3u8
```

(This is illustrative – FFmpeg supports segmenting output for HLS and producing variant playlists). The transcoding Job container will run such a command. We ensure the container has a volume (PersistentVolume or an EmptyDir that is later collected) to output the `.m3u8` and `.ts` files. Likely, we’ll have it write to a shared storage (like an object storage or a mounted PV) so that the Streaming service or a CDN can serve the files. Alternatively, the transcoder could, after encoding, upload the segments to cloud storage (like S3 or GCS). In a cloud-agnostic way, an on-cluster distributed filesystem (or S3 via MinIO) could be used as the target.

**Resource Management:** Transcoding is heavy, so we must manage cluster resources:

* We label certain nodes as “transcode=enabled” and use **node selectors or taints** so that transcode jobs either run on specific nodes (maybe ones with more CPU or not interfering with interactive workload). We also set resource **limits** to ensure one job doesn’t consume more than allocated CPU, and use **priority classes** if we want these jobs to not starve more critical system pods.
* We use **Horizontal Pod Autoscaler** or custom scaling to spin up more worker pods when needed. Specifically, we implement a custom **autoscaler based on queue backlog**. E.g., if 10 TranscodingJob CRs are pending and only 2 are running (due to some concurrency limit), we can scale out. Egnyte did this: *“custom metrics-based autoscalers look at the current backlog of the transcoding queue and deploy more jobs as needed; scale down when backlog reduces”*. In Kubernetes, an HPA can’t directly scale Jobs (since Jobs are one-off), but we could maintain a Deployment of a certain number of worker pods reading from a queue. However, with our approach of one Job per video, the autoscaling is more about provisioning nodes. If using a cluster autoscaler, the pending Jobs will cause new nodes to spin up automatically if resources are insufficient.
* Alternatively, we might have a single long-running Transcoder deployment that pulls tasks from NATS or Redis queue and processes them. That is a simpler design but less elastic than using one Job per task. However, using the operator approach, one could argue the operator itself is managing the “queue” via CRs, and K8s schedules pods accordingly. We choose the one-Job-per-task model for clarity and isolation.

**Handling Failures & Retries:** If an FFmpeg process fails (exit code non-zero), the Kubernetes Job will be marked Failed. We set `.spec.backoffLimit` on the Job to maybe 1 or 2, so it can retry automatically a couple times (in case of transient errors). The Transcoding controller watches for a Job failure; if it exceeds retry limit, it will mark the TranscodingJob CR as Failed with the error message (we can retrieve pod logs or exit code). Additionally, the controller can implement **stuck job detection**. Egnyte’s transcoder for example had issues with hanging FFmpeg, so they implemented health checks: *“If FFmpeg job is not producing any more HLS segments but still running, or if no new message was picked up for a while when backlog exists, the container fails its health check, causing Kubernetes to restart it.”*. We can do similar: the FFmpeg wrapper script can periodically output progress or touch a file; a liveness probe in the pod checks that progress. If no progress for X minutes, kill the pod (K8s will retry or mark failed). This prevents zombie processes eating CPU with no output.

When a pod restarts or fails, we want the job to possibly resume or at least not hold the system stuck. If a transcoder pod restarts, any partial output might be unusable; we might configure things to start over clean or perhaps to resume from last keyframe (though FFmpeg doesn’t resume partial easily unless we segment it and skip done segments). Simpler is to start over. If it fails repeatedly, we alert (via event or metric).

**Job Queuing vs Instant Spawn:** We don’t want to overload the cluster by starting too many transcodes at once. We can impose a limit by controlling concurrency in the operator: e.g., only allow N jobs running concurrently (others remain in Pending status at the CR level). Alternatively, rely on Kubernetes’ own scheduling and resource requests to naturally queue pods (if you request 4 CPU but only 8 CPUs available, at most 2 pods run, others stay Pending until resources free up). This natural backpressure might suffice. To be explicit, we could label pending CRs as such and not create new Job objects if too many already active.

**Integration with Downloads:** The Transcoding service likely subscribes to an event or is triggered when a download completes. We ensure that the large video file is accessible to the transcoder. If both download and transcode happen in the cluster, one approach is to have them share a PersistentVolume: the Download job saves the file to PV, and then Transcode job reads from PV and writes outputs. Another approach is the download service could, after completion, push the file to an object storage, and the transcoder pulls from that (via HTTP or S3 API). For speed, a local PV might be best. But if using cloud storage, it simplifies sharing and we just need the URL.

**Lifecycle and Cleanup:** After a successful transcode, we have a bunch of segment files and playlists. We will likely keep them in storage for serving (so not delete immediately). If using PVs, perhaps each job gets its own emptyDir (ephemeral) and at the end we upload the results to permanent storage (like move to an NFS share or push to object store) and then the pod dies and data is gone from container disk. Or we mount a common output volume so that files persist. We need to decide: if on-prem or self-hosted, maybe we use a shared NFS or Gluster volume mounted into transcoder pods for outputs; if cloud, maybe directly upload to S3 via AWS CLI in the container after encoding. For cloud-agnostic, using a **MinIO** (S3-compatible) within cluster as an object store might be a good solution – transcoder can upload to it using S3 API, and streaming service can fetch from it (or serve from it directly). This way, no dependency on external cloud storage.

We also manage retention: storing multiple renditions takes space (transcoded outputs might be 20-50% size of original per rendition). We consider cleaning up old transcodes if, say, the original was removed. Possibly a periodic job or finalizer on media deletion to remove derived files.

**Observability for Transcoding:** Each job pod can push metrics (like frames per second, ETA) to Prometheus. We could run an **FFmpeg stats filter** writing to stdout and have a small parser to emit metrics. However, given the short life of pods, we might rely on logs for detailed info and just have high-level metrics in the operator (like number of active jobs). The operator can maintain a metric of backlog and active count, which we feed to autoscaler and monitoring.

**Autoscaling and Cost:** Because transcode jobs are expensive, we integrate cluster autoscaling. If a surge of videos come in, new nodes can spin up (perhaps using cheaper spot instances if on cloud). Egnyte’s story: *“Using preemptible (spot) VMs for transcode jobs saved cost, with logic to handle their evictions gracefully”*. We could mark our pods as suitable for running on spot instances by node affinity or tolerations, ensuring cost optimization.

**CronJobs:** Aside from on-demand transcodes, we might have scheduled tasks, e.g., a **CronJob** to periodically clean temp files or to re-transcode content if policies change (not common). We can also have CronJobs for generating preview thumbnails or periodic health checks. Kubernetes CronJob would manage those on schedule.

**FFmpeg Tuning & Error Handling:** We run FFmpeg with flags to ensure we get exit on error. We capture stderr logs (which can be quite verbose). The controller might fetch logs on failure to include a brief error (e.g., “Unknown codec” or “Out of memory”) in the CR status for troubleshooting. We also set environment variables to control thread count if needed (FFmpeg by default will use multi-threading; with 4 CPUs allocated, it will likely spawn threads accordingly, which is fine).

**Pipeline Enhancements:** If needed for performance, we could implement parallel chunk encoding – e.g., split a video into segments and have multiple pods encode different segments then combine. However, that adds complexity (must stitch segments seamlessly). Since real-time is not a strict requirement (we do async), we can avoid that complexity and stick to one file per job.

**Testing Transcoding:** We’ll have sample media files to test the pipeline (maybe a known Big Buck Bunny clip). We ensure our container works similarly on different environments (we might use the same container locally via Docker to test encoding output).

By using Kubernetes Jobs and careful monitoring, our transcoding system is **scalable and robust**. If demand increases (e.g., many videos to process), we simply see more Job objects pending which triggers autoscaling. If the cluster is full, jobs queue up (either explicitly via backlog or implicitly by unscheduled pods), which is fine – they complete when resources free up. The event-driven linking (Download done -> create Transcode job) ensures we don’t lose tasks.

We also implement **notifications**: once a transcode is done, the Transcoding service (through the controller or job completion hook) can emit an event `TranscodingCompleted` with the media ID and maybe available bitrates. The Media Library listens and updates the media status (now playable). The frontend could also get a signal (via WebSocket or polling) to know that a previously “processing” item is now ready.

This architecture mirrors what Egnyte described: they deployed FFmpeg in K8s, found the 4 CPU sweet spot, used custom autoscalers on backlog, and health checks to kill stuck jobs. We’ve taken those lessons into account to ensure production-grade robustness and cost-efficiency.

## 7. API Gateway & Service Mesh Integration

To expose our services externally and manage cross-service concerns, we use an **API Gateway** pattern in front of our microservices, and internally we leverage a **service mesh** for secure and reliable service-to-service communication.

**API Gateway (gRPC-Gateway):** Since our backend services use gRPC, we cannot directly call them from a web browser (which can’t natively speak gRPC over HTTP/2 without the gRPC-Web protocol or a proxy). We implement a REST/HTTP JSON gateway for external clients using **grpc-gateway** (which translates REST calls to gRPC behind the scenes). For each gRPC service that needs external exposure (likely a **Gateway/BFF service** or selectively the Media service for metadata, Streaming service for content access), we add HTTP annotations in the proto. For example:

```proto
service MediaService {
  rpc GetSeries(GetSeriesRequest) returns (GetSeriesResponse) {
    option (google.api.http) = {
      get: "/api/v1/series/{id}"
    };
  }
  // ... other RPCs
}
```

The grpc-gateway generator uses these to produce a reverse-proxy HTTP server. We deploy this as part of the service (the service binary can serve on two ports: one for gRPC, one for HTTP, or just one port with both muxed if configured). Alternatively, we can run a separate deployment for the gateway (e.g., Envoy proxy with gRPC-JSON transcoding filter). The gateway ensures that external RESTful calls (from the browser or third-party integrations) can interact with our system easily. It also allows features like Swagger documentation – the grpc-gateway can output an OpenAPI spec from the proto annotations, which we can use to document our API for frontend devs.

The API Gateway also performs **routing**: based on URL paths or hostnames, it directs requests to the appropriate internal service. If we use Kubernetes **Ingress** or a Layer 7 proxy (like Istio’s ingress gateway or an Ambassador API gateway), we configure routes: e.g., `/api/v1/stream/` goes to Streaming service, `/api/v1/library/` goes to Media service, etc. If using a dedicated gateway service (like **Kong, KrakenD, or Envoy**), we can define those mappings. In simpler setups, an Nginx Ingress with host-based routing might suffice (e.g., media.example.com routes to Media gRPC gateway service).

**Security & Auth:** The gateway is a good choke-point for authentication and authorization of incoming requests. For instance, we can require a JWT on all calls. We could integrate with an OpenID Connect provider such that the gateway validates JWTs (using middleware or Envoy filters) and possibly performs OAuth2 token introspection if needed. This offloads auth from each microservice. Alternatively, each service could also do auth via interceptors (as discussed earlier), but doing it at gateway avoids duplication and ensures non-gRPC clients are handled. We likely still include auth in the services for internal calls (e.g., if one service calls another, that might carry a service JWT or mTLS identity, see mesh below).

We enforce **rate limiting** at the gateway to protect against abuse. This could be done via a rate-limit service (Envoy supports a global rate limiting service integration) or simple local rate limits (like "each IP max N requests per minute"). For instance, Envoy can be configured with a descriptor-based rate limit: maybe 100 requests/min per auth token on metadata endpoints. Since streaming segments are static content ideally, we might rely on CDN or browser caching for that rather than gating via API gateway beyond the initial playlist request.

**Service Mesh (Istio or Linkerd):** Within the cluster, we deploy a service mesh to handle service-to-service communication aspects like encryption, observability, and reliability. We can use **Istio** (powerful, feature-rich) or a lighter-weight **Linkerd** (ultra-simple mTLS and metrics). Given we want features like circuit breaking, traffic routing policies, and maybe advanced telemetry, Istio is a good choice. Each service’s pod will have a sidecar proxy (Envoy in Istio). The mesh auto-configures **mTLS** between proxies, meaning all service traffic is encrypted in-cluster. This addresses security for internal APIs without each service needing TLS logic. Istio can enforce that only certain services can talk to others (authorization policies) if needed.

The mesh also collects metrics: Envoy sidecars emit standardized metrics (like HTTP/gRPC request counts, latency, etc.) and can send tracing spans to Jaeger. So even if a service doesn’t instrument a particular client call, the sidecar will report it. **Distributed tracing** headers (like `x-b3` or W3C Trace-Context) can be automatically propagated by Envoy or we can propagate in application – doing both ensures full coverage.

**Traffic Management:** Istio allows complex traffic shifting rules via VirtualServices and DestinationRules. We can implement canary deployments (e.g., route 5% traffic to new version), A/B testing, etc., with simple config, rather than coding it. Also, we can do **URL-based routing** at the ingress: the Istio IngressGateway can serve as our API gateway actually, with a VirtualService routing paths to services. For example, we define:

```yaml
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata: { name: api-gateway }
spec:
  hosts: ["myapp.example.com"]
  gateways: ["public-gateway"]
  http:
  - match:
      - uri: { prefix: "/api/v1/library" }
    route:
      - destination: { host: mediasvc.default.svc.cluster.local, port: 80 }
  - match:
      - uri: { prefix: "/api/v1/stream" }
    route:
      - destination: { host: streamingsvc.default.svc.cluster.local, port: 80 }
  # etc.
```

This way, we might not even need a separate grpc-gateway if we decided to use gRPC-Web + Envoy at the edge – but grpc-gateway probably simpler for front-end.

**Circuit Breakers:** The mesh provides out-of-the-box circuit breaking and retry policies at the network level. We configure DestinationRules for each service to prevent overload. For instance, we can say “calls to MediaService: max 100 concurrent connections, overflow gets queued or rejected; if a service instance has 5 consecutive failures, trip circuit for 2 minutes” (Envoy outlier detection). As Istio docs say, *“Istio enforces circuit breaking limits at the network level using Envoy sidecars, so you don’t need to bake it into each app.”*. We will set sensible defaults: e.g., no more than X pending requests per service instance to avoid queueing too many, and enable outlier detection to eject unhealthy pods from load balancing (Envoy will stop sending traffic to a pod that returns too many 5xx errors, for example). This improves resilience: if one service instance is misbehaving (e.g., stuck or slow), Envoy will route around it after a threshold, rather than all calls hanging.

**Retries and Timeouts:** We can also configure the mesh to automatically retry certain calls (with limits) and apply timeouts. For example, a VirtualService can say “if MediaService call fails with a 503, retry once after 0.5s”. We should be cautious to not double-retry at both client library and mesh; we’ll coordinate these. Often it’s easier to let Istio handle network errors and keep application logic simpler.

**Service Discovery:** With the mesh, services refer to each other by hostnames (Kubernetes service DNS) and the sidecars intercept traffic. This gives us benefits like dynamic service discovery and load balancing at the connection level. We do not have to hardcode IPs; when scaling pods, Envoy picks up endpoints from Istio’s control plane.

**Monitoring and Telemetry:** Istio (or the mesh) will report metrics such as HTTP request count, latency distribution, etc., to Prometheus. We will have dashboards showing per-service success rates and latencies. Also, distributed traces combining spans from services and the proxies will help debug performance issues. If a call is slow, we can see if it’s network or the server by looking at trace timeline.

**Security Policies:** Using the mesh, we enforce **mTLS** – all service traffic is encrypted and authenticated (the proxies use certificates). We can also define an Istio **AuthorizationPolicy** if needed, e.g., to restrict that only the Gateway can call certain services from outside (though network wise only gateway exposes external). We might say that Download service can only be called by Media service, etc., if that made sense, but likely unnecessary within a small team.

**Edge Termination:** The Istio ingress (or whichever gateway) will terminate TLS from users (we’ll have a valid TLS cert on the domain). Then it can either use mTLS to target sidecars or just normal since internal is already mTLS between sidecars.

**Service Mesh vs Simpler:** If for any reason we decide not to go full mesh (maybe complexity), an alternative is to use just the API gateway and internal gRPC calls with manual TLS. But given we want robust observability and security, mesh adds value. Also, our team will add **service mesh expertise** documentation and maybe use **automatic sidecar injection** to gradually add it.

We should note that **gRPC traffic in Istio** is supported and the proxies understand HTTP/2 frames, so it’s fine. We might disable Envoy’s telemetry on high-throughput streaming of HLS segments (if we were to route segment files through the mesh) to reduce overhead. But if HLS is served via separate Nginx or CDN, not an issue.

**Developer Experience:** We hide the complexity from devs by providing a `kubectl port-forward` or a local proxy for testing if needed. Also, we maintain config in Git (Istio VirtualServices, etc., as part of our deployment manifests) so it’s reproducible.

In summary, the **API Gateway** handles external requests, translating protocols and consolidating routes, while the **service mesh** provides internal **mTLS, load balancing, retries, and observability** in a uniform way. This combination ensures that our microservices ecosystem is secure, resilient, and easier to monitor, without each service needing bespoke solutions for these concerns. It also future-proofs us: for example, if we deploy a new version of a service, we can use the mesh to do a canary rollout controlling traffic split without changing client configs.

## 8. Observability & Debugging Architecture

Building a production-grade system requires excellent **observability** – we need to be able to trace requests, inspect logs, measure performance metrics, and quickly debug issues across dozens of microservices. We implement a comprehensive observability stack:

**Structured Logging:** Each service uses structured, contextual logging, favoring a high-performance logger like **Zap** or **Zerolog**. Both provide JSON output and are extremely fast (Zerolog being zero-allocation and the fastest, Zap a close second). We’ll choose one (say Zap for its simpler API and widespread use in Kubernetes projects, or Zerolog for absolute speed). Logging is done in **structured key-value format** (JSON) so that log aggregators (like Elastic or Loki) can index fields. We include fields such as `service`, `trace_id`, `request_id`, `user_id`, etc. A **correlation ID** (aka request ID) is critical: we generate a UUID for each incoming external request (if not already present) and propagate it through internal calls (e.g., via gRPC metadata). This ID is added to the context and our logging interceptors log it on every message. For example, we might use Zap’s ability to create a child logger with context fields: *“create a child logger containing the correlation ID so that it appears in all subsequent logs for that request”*. We implement a small middleware in each service that does:

```go
rid := getOrGenRequestID(ctx)
logger := baseLogger.With(zap.String("req_id", rid))
ctx = context.WithValue(ctx, loggerKey, logger)
```

Now all deeper in the call use this logger from context. This way, when we search logs, we can filter by `req_id` and see the complete flow across services (since we propagate the same ID via gRPC metadata and each service includes it in logs).

We also log important domain events and decisions. E.g., when a transcode job starts or completes, we log an INFO with media ID and outcome, so ops can trace what happened when. Errors are logged with stack traces if possible (Zap can attach stacktrace for error-level logs). We avoid logging sensitive data (no plain passwords, etc.), and if needed we can use Zap’s field filters to omit certain keys or hash them.

Our log retention strategy is to send logs to a central system. In Kubernetes, we can deploy **EFK (Elasticsearch-Fluentd-Kibana)** or use a cloud log service. Alternatively, **Loki** (Prometheus-style logging) is lightweight and works well with Grafana. Fluent Bit as a daemonset can collect logs from stdout of pods and push to Loki/Elastic. We tag logs with service name and environment.

**Metrics (Prometheus & OpenTelemetry):** Each service emits metrics about its behavior. We integrate **OpenTelemetry** to unify tracing and metrics instrumentation. For metrics, we use Prometheus as the collector. We expose a `/metrics` endpoint (if using grpc-gateway, we can mount it or run a separate HTTP port) with Prometheus format. This usually comes from using the Prometheus Go client or OpenTelemetry metrics SDK. Key metrics:

* **Business metrics:** e.g., number of videos processed, size of media library, current downloads/transcodes in progress, etc.
* **Performance metrics:** request counts, error counts, latencies for each RPC (these can be automatically captured by gRPC interceptors or by Envoy sidecar).
* **Resource metrics:** we can rely on cAdvisor/K8s for CPU/memory, but application can expose things like goroutines count, queue lengths, etc. For example, if we internally queue transcoding tasks, we’d expose gauge of queue depth (though in our design K8s jobs handle queueing).

We make heavy use of **OpenTelemetry tracing** to track end-to-end requests. We instrument gRPC calls with OpenTelemetry’s gRPC interceptors (otelgrpc). This will create a trace span for each server and client RPC automatically. We configure an **OpenTelemetry Collector** in the cluster to receive spans and forward them to a tracing backend like **Jaeger** or **Grafana Tempo**. Each incoming request from the client might start a trace (the gateway or ingress could start one). Then as it calls Media service, a new child span is created, then maybe a child span for a DB query, etc., giving a tree of the request. If an operation triggers an async event, we propagate the trace context with the event so that the processing of that event can be linked to the originating trace (there are patterns to continue traces across message queues; if not, at least we log correlation IDs to connect them manually).

In addition, we ensure to **correlate logs with traces**. One way is to log the trace ID and span ID in each log entry (which we can get from context via OpenTelemetry APIs). Some logging libraries can integrate with OpenTelemetry to automatically add trace IDs to logs. This means if we see an error log, we can find the trace ID in it and then go to Jaeger UI and see the entire distributed trace around that error – super useful for debugging complex issues.

**Monitoring Dashboards & Alerts:** We set up Grafana dashboards for key metrics, like:

* Throughput of requests per service, error rate (with red highlighting if error % > some threshold).
* Latency histograms for gRPC calls.
* Resource usage (via Prometheus Node Exporter or K8s metrics).
* Custom metrics like “Transcoding Backlog” (how many videos waiting to be processed).
* Business metrics like “Total Media Items in Library” or “Active Streams”.

We also set up **alerts** in Prometheus for critical conditions: e.g., if error rate for a service spikes, if a particular job has been running unusually long (we can expose a metric or use K8s Job age), or if disk space in the volume for video storage is low.

**Remote Debugging:** In a secure production environment, we can’t easily attach GDB, but we can facilitate **remote debugging in non-production** clusters. For example, we might allow port-forwarding Delve (Go debugger) to a pod in a dev environment to step through code. More practically, we rely on detailed logs and traces in prod, but also set up the ability to capture **profiles**. We compile services with the `net/http/pprof` handler enabled (protected so only accessible internally). This way, if CPU usage is high or memory leak suspected, we can port-forward to a pod’s pprof endpoint and grab a profile or heap dump. We also consider using **Grafana Pyroscope or Parca** for continuous profiling in production to catch hot spots over time.

**Chaos Engineering & Testing:** To build confidence, we incorporate chaos tests. We might use **LitmusChaos** or **Chaos Mesh** to inject failures like pod crashes, network latency, etc., and ensure the system self-heals and remains stable. For example, deliberately kill a Transcoding pod mid-job – does it retry correctly? Or add network latency between services – do our timeouts and circuit breakers operate to avoid cascading failure? We schedule these experiments in staging environments, and possibly run small scale ones in prod (very carefully) as part of resilience testing.

**Performance Bottleneck Detection:** We will do load tests (see Testing section) and use traces to see where latency accumulates. If database queries are slow, traces (with spans around DB operations) will highlight that. We also watch metrics like 99th percentile latency and if it grows, we investigate with tracing or profiling. For micro-optimizations, Go’s built-in profiling (pprof) can be used offline – e.g., run a staging environment under load and then run CPU profile to find functions consuming time. We also track garbage collection metrics (like GC pause time, available via Prometheus client). If GC is an issue (due to many short-lived objects), we consider optimizations such as pooling.

One particular area to watch is the **FFmpeg pods**: they are heavy on CPU. We monitor at node level to ensure they get the CPU they request and don’t cause node thrashing. We collect their metrics if possible (e.g., FFmpeg can output encoding fps, we can capture that in logs at intervals).

**OpenTelemetry Collector**: We likely run a centralized collector that receives OTLP data (spans, metrics) from services and proxies. The collector can then send to Prometheus (metrics) and Jaeger (traces) or to a vendor like New Relic, etc., if ever needed. This decouples our code from any specific backend; we just use Otel SDK. We set sampling rates for tracing such that maybe a small percentage of requests are fully traced (or use tail-based sampling to keep traces that had errors). We definitely always trace requests that resulted in errors, to diagnose them.

**Incident Response:** We implement structured logs and metrics such that if something goes wrong, it’s easy to pinpoint. For instance, if a user says “video playback failed at 8pm”, we can search logs for that media ID or request around that time, find a trace ID, open the trace and see that perhaps the Transcoding service threw an error code at that time. Or see maybe it was slow due to a DB call. This observability triad (logs, metrics, traces) is key to quick root cause analysis.

**Dashboards & Logging UI:** We set up Kibana or Grafana Loki’s log UI for searching logs by fields (service, severity, correlation id). Grafana shows metrics, and Jaeger UI (or Grafana Tempo UI) shows traces. We integrate these: e.g., clicking a trace ID in logs opens the trace. This integration is often available out-of-box if IDs are consistent.

**Audit Logging:** For security, we also maintain some audit logs – e.g., any deletion of media or changes in settings can be logged at INFO with an "audit" tag, so we can filter those if needed.

**Third-Party Monitoring:** The system being cloud-agnostic doesn’t rely on cloud-specific monitoring, but if on a cloud, we could integrate with cloud monitors (like CloudWatch, etc., via exporters). However, our chosen stack is primarily open-source self-hosted monitoring which avoids lock-in.

Finally, we plan for **capacity monitoring**: metrics will show how utilized our cluster is (CPU, memory, disk). If metrics show e.g. transcoding jobs consistently queued or high CPU, that drives scaling decisions (maybe upgrade nodes or add more nodes). We set up alerts if, say, CPU usage is above 80% for 15 minutes or disk > 90% full, to proactively manage resources.

In summary, our observability architecture ensures that we have **logs with correlation**, **metrics for performance and business KPIs**, and **distributed traces** for every user journey, all feeding into a coherent monitoring system. This greatly aids debugging: whether it's a slow response, a mysterious error, or verifying that a new release doesn’t degrade performance, we have the data to investigate and the tools to visualize it.

## 9. Testing Architecture

Quality is enforced through a multi-layered **testing strategy** covering everything from individual functions to full end-to-end scenarios.

**Unit Testing (Logic Tests):** All services have extensive unit tests for their domain logic. We follow Go best practices for unit tests:

* Use **table-driven tests** to cover multiple cases succinctly. For example, for a function computing next episode number, we might have a table of inputs and expected outputs, then loop with `t.Run` for each case.
* We use lightweight **mocks or stubs** for external dependencies. Rather than deploy a real DB for unit tests, we define interfaces (as we did with repositories) and in tests provide a stub implementation (or use a mocking framework like **golang/mock** to generate a mock that records calls). For instance, to test the MediaService’s `AddSeries` logic without a real DB, we create a fake repository that just records the input or returns a preset result. We verify that after calling the service method, the repository was called with expected arguments (using the mock’s expectation mechanism).
* We test edge cases: invalid inputs (should return gRPC InvalidArgument errors), boundary values (e.g., zero episodes, maximum episodes).
* We also incorporate **property-based testing** for certain pure functions. For example, if we have a function that generates a slug or does date parsing, we can use Go’s `testing/quick` or the **Gopter** library to generate random inputs and verify certain invariants hold.
* We run tests with `-race` to catch any race conditions in concurrent code (especially important for things like events or caches).
* We aim for a high coverage on core business logic (though we care more about covering important scenarios than blindly hitting coverage %).

**Integration Testing (Service tests):** We then test each service in more realistic conditions:

* **gRPC integration tests:** We launch the gRPC server (in-process) and use a real gRPC client to call it, verifying the full request-through-handler flow. For this, we might instantiate an in-memory or local version of dependencies. For example, spin up a lightweight SQLite database or a Dockerized Postgres for the Media service integration tests, so that repository calls actually go through the SQL driver. We can automate this with **Testcontainers-Go** which is a library to programmatically start Docker containers in tests. For instance, at test startup, use testcontainers to launch a Postgres container, run migrations, set the DSN in the service config, then run tests against it. After tests, the container is terminated. Similarly, we can start a local NATS server in a container for testing event publishing.
* **External dependencies:** For things like the Download service reaching an external URL, in tests we can run a dummy HTTP server (perhaps using **httptest.Server**) that simulates a file download endpoint. Or if testing integration with a third-party API (like metadata), we can use WireMock (there is WireMock + testcontainers even for gRPC as noted).
* **Transaction and DB tests:** We write tests that use the real DB schema (applied via migrations) and verify that e.g. the repository’s SQL queries actually retrieve the right data. Tools like **sqlfixtures** or just manual inserts help set up initial data. These integration tests ensure our SQL and ORM mappings work as expected (for instance, verifying that GORM relationships are loaded, or that an Ent client writes the right rows).
* **Integration of multiple services:** We might spin up two services together and let them interact via gRPC in a test. For example, test that when Download service finishes and publishes an event, the Transcode service receives it (this would require also running a NATS in background for test). We could simplify by not actually using NATS in the test but injecting a mock event bus that captures published events and then directly calling the Transcode service’s handler. However, for true integration, using testcontainers to run a NATS server is feasible (NATS is lightweight).

To orchestrate multi-container integration tests, we can use **docker-compose** in tests or Kubernetes in CI (like `kind` cluster) – but that approaches E2E territory. A simpler path is a Go test that:

* starts NATS container,
* starts Postgres container,
* starts instances of our services (as Go routines or separate processes),
* then simulates a scenario: e.g., create a Media entry, simulate a download complete event, see that transcode job is triggered, etc.
  This is complex but doable for CI/CD given enough resources/time. We might reserve full multi-service integration for a staging environment test rather than unit test phase.

**End-to-End (E2E) Testing:** These tests treat the whole system as a black box and verify user flows:

* We could use a **Kind (Kubernetes-in-Docker)** cluster in CI to deploy the full system (perhaps scaled-down) and then run a test suite against the external API. For example, deploy all services and infrastructure (maybe using our Helm charts or manifests), then run a test that calls the public API: “Add Media” (perhaps our API to add a URL to download), then poll for status, then attempt to play stream, etc. We verify outcomes (like an HLS master playlist becomes available).
* Alternatively, we use **contract testing** for each service’s API. With gRPC, one approach to contract testing is to ensure the proto definitions match what consumers expect. Since we share protos between frontend and backend, this is inherently consistent, but if there were external clients, we’d formalize an API contract. We can also test that our grpc-gateway JSON API meets expectations by writing tests using the OpenAPI spec or using something like **Dredd** or **Pact** for provider verification. For instance, the frontend team might write Pact consumer tests (in JS) expecting certain REST responses; we run those against the live service in CI to ensure no breaking change.

Given our focus on being **production-grade**, we likely set up a **CI pipeline** that does:

1. Linting and unit tests.
2. Build containers.
3. Integration tests using containers (with testcontainers or docker-compose).
4. Deploy to a ephemeral test cluster (maybe using Kind in GitHub Actions, or a staging namespace in a real cluster).
5. Run E2E tests (could be a series of curl or grpcurl commands, or a Go test suite that uses the client stubs to call through the gateway).
6. Possibly run a security scan (like dependency scan or container vulnerability scan with **Trivy**) at the end.

**Test Tools & Frameworks:** We use **testify** for assertions in Go tests (it’s quite standard and makes output nicer). For mocking, **gomock** or **testify/mock** can generate mocks from interfaces, which we use especially for services that call others (we can mock the gRPC client interface to simulate how a dependency service would respond). For example, test the Media service’s logic when Transcoding service responds with a certain status.

We also incorporate **Testcontainers** heavily as mentioned, because it provides a programmatic way to ensure, for example, that “StartPostgres(t)” returns a connection URL to a fresh Postgres with latest migrations. This avoids an external dependency on CI environment.

**Load Testing:** As part of testing (though often done outside CI), we perform load and performance tests. We use a tool like **k6 (Grafana k6)** for HTTP (it now supports gRPC testing via a plugin). We might also use **ghz** for gRPC which is a dedicated load test tool. We simulate scenarios: e.g., 100 concurrent clients requesting HLS playlists and segments, or bursts of 20 large videos added at once to see how the system queues them. These tests reveal bottlenecks – e.g., maybe the Media service DB saturates at X QPS, or the gateway becomes CPU bound. We use results to adjust resource requests and maybe code (like adding caching or adjusting concurrency).

**Chaos Testing in Staging:** We incorporate some chaos experiments in a staging setup. For example, using a tool like **chaoskube** to randomly kill pods and ensure the system (with its mesh and retries) continues serving. We can automate some of these in integration tests by actually killing a container in the middle of an E2E test and verifying the client eventually succeeds via retry.

**Testing Resiliency of Events:** We test event handling by simulating message loss or duplication. For example, in a test, publish the same event twice and see that the system handles the duplicate gracefully (e.g., either ignores the second or processes idempotently). Or drop an event (perhaps by shutting down NATS briefly) and see if the system can recover (if we use JetStream with persistence, it shouldn’t lose it, but if we did ephemeral, maybe we rely on eventual consistency or periodic reconciliation to recover). These scenarios ensure robustness.

**Frontend Testing:** While the question focus is backend, we should mention the **frontend** (React/TypeScript) also needs testing:

* **Unit tests** for UI components (using Jest, React Testing Library).
* **Integration tests** using a framework like Cypress or Playwright that run a headless browser against a running dev server (maybe hitting a mocked backend or a real staging backend).
* Because the frontend uses gRPC-web, we might use mocks for the gRPC calls in tests (there are libraries to mock gRPC-web or we can stub the generated client).

We make sure to test key front-end behaviors: state management (Redux store updates correctly on actions), real-time updates (if we use web socket or streaming, test that adding a new media triggers UI update within a time bound), and error handling (e.g., simulate server errors and ensure the UI shows a message).

**Code Quality Checks:** Before tests, we run **golangci-lint** which includes not only style and trivial issues but can catch bugs (like unused variables, nil pointer potential, etc.). We also run **gosec** to catch any insecure code patterns (like use of `os.Exec` with variable input, or insecure random usage). These tools gate the CI so we don’t merge code that fails linting or basic security checks. We also might run **Go Fuzz** tests for critical parsing logic (fuzz testing is now integrated in Go for finding panics or crashes with random input).

**Test Artifacts & Coverage:** We measure test coverage and aim for e.g. 80%+ on core packages. We treat reduced coverage as a flag to add tests, but not as an absolute gate if the untested parts are trivial or generated code. We have CI produce coverage reports, and maybe use a tool to comment on PRs if coverage drops.

**Continuous Testing in CI/CD:** Every pull request triggers the above pipeline. We also have scheduled runs for longer tests like load tests (maybe nightly) and security scans. Before deploying to production, we deploy to a staging environment where a suite of automated smoke tests run against the new version (as a final sanity check). Only then promote to production.

**Test Data Management:** We use small sample media files (e.g., a 1-minute video clip) for testing transcoding to keep tests fast. We include them either in a testdata folder or generate them (we can generate a dummy video with FFmpeg during test initialization if needed). For integration tests with media, using real media ensures our FFmpeg pipeline is working (e.g., we can verify that output .m3u8 has expected entries).

By layering tests (unit -> integration -> e2e) we ensure each piece works in isolation and the system works as a whole. This gives confidence that changes (refactoring or new features) won’t break existing functionality. Moreover, when bugs are found in production, we will write a test reproducing it (if possible in a controlled env) to prevent regressions.

## 10. Frontend Architecture Patterns

The frontend is a **TypeScript** application (likely React given mention of hooks and Redux). It communicates with the backend via **gRPC-web** or REST. We design the frontend with modern patterns to ensure maintainability and responsiveness.

**State Management:** For global/stateful information, we use **Redux Toolkit** (RTK) or **Zustand** depending on complexity. **Redux Toolkit** is a popular choice that reduces the boilerplate of Redux and comes with good DevTools integration (time-travel debugging, etc.). It’s described aptly as *“the artisan’s toolkit for state management…streamlines complexities of Redux, making state management smooth and delightful.”*. We use Redux for app-wide state: user auth info, library of media (if caching in client), UI preferences, etc. We structure our Redux slices by domain (e.g., a slice for “mediaLibrary”, a slice for “player”). RTK Query (part of Redux Toolkit) can be used to handle data fetching and caching of API results; however, RTK Query is more REST/HTTP oriented. There are community solutions to use it with gRPC by wrapping the calls in baseQuery, or we may just use normal thunks for gRPC calls.

**Zustand** is an alternative lightweight state container which can be convenient for smaller or more localized state. We might use Zustand for parts of the app that don’t need the full Redux apparatus. For example, if there’s a complex component with many interactive parts (like a video player controls state – play, pause, etc.), Zustand could manage that as a self-contained store with hooks, without involving the global Redux store. Zustand is hook-based and has minimal overhead.

It’s possible to mix: use Redux for global state and use context or Zustand for local component states where appropriate. We aim to avoid overloading Redux with every bit of component state (which was an antipattern in old days).

**gRPC-web Communication:** We decide how the frontend calls backend:

* If using **grpc-web** protocol, we need a proxy (like Envoy) or the backend has integrated grpc-gateway. Assuming we have grpc-gateway exposing REST, the simplest approach is to use **RESTful JSON calls** from the frontend (with `fetch` or Axios), using the endpoints provided by grpc-gateway. This sacrifices some type safety (we could generate a TS client from OpenAPI or from proto using something like **protobuf-ts** which can also do grpc-web, or simply define TS types matching API).
* Alternatively, use **grpc-web client libraries** directly. The `grpc-web` npm package provides a client that can call methods defined in the proto via Envoy proxy. We can generate `.pb.ts` and `.pb.grpc-web.ts` files using protoc with `--js_out=import_style=commonjs+dts` and `--grpc-web_out=...`. These give us strongly-typed client stubs in TS. The frontend then calls, say, `MediaServiceClient.getSeries(request, {}, (err, resp) => { ... })` or the Promise-based variant if using an updated library. This is very nice type-wise, but requires the Envoy proxy or a gRPC-web enabled gateway.
* Considering simplicity, we might just go REST via grpc-gateway since it doesn’t require special handling in deployment (Envoy etc.). But if we want to embrace gRPC end-to-end, we set up Envoy or use the grpc-web npm lib with our own small proxy (the Torq article mentions using Envoy or their own Go proxy for grpc-web).
* The **BFF (Backend-for-Frontend) pattern** is mentioned in Torq’s case – they moved to an API Gateway that frontends everything. In our case, the grpc-gateway or ingress can serve that role.

We ensure any generated TS code (either via grpc-web or OpenAPI) is integrated in the frontend build. Possibly we have a step in CI that publishes a small NPM package of our proto types for the frontend to import (Torq did something similar, generating an npm package of their gRPC client code).

**Component Architecture:** We use functional components with **React Hooks** for state and side-effects. Complex components are broken down with **compound component pattern** where appropriate. For example, a MediaItem component might be composed of sub-components \<MediaItem.Title>, \<MediaItem.Thumbnail> that share state via context implicitly. This allows flexible composition in JSX.

We leverage **render props** or more modern **custom hooks** for reusing logic. For instance, to handle streaming updates, we might write a custom hook `useTranscodingProgress(mediaId)` that internally sets up a subscription (like opens a gRPC server-stream or WebSocket) and provides progress state to components. This hook encapsulates the reconciliation logic (e.g., if connection drops, retry after backoff) and the component using it just gets the latest progress.

**Real-time Updates:** There are a few ways to get real-time updates on the frontend:

* **Server-Sent Events or WebSockets:** We could have the gateway or a dedicated service push events to the client when, say, a transcode finishes or progress changes. Perhaps easier is a WebSocket where the client subscribes to updates for a media ID. Since our backend is mostly gRPC, an alternative is to use gRPC-web’s support for server-streaming. grpc-web can handle server-stream responses (it buffers them a bit differently, but it’s supported). So we could implement a gRPC server-streaming call like `WatchTranscodingStatus(mediaId)` that the client calls via grpc-web. Envoy will chunk-transfer encode it to the browser. The React app would then get callbacks on each message (progress %) until completion.
* If setting up that is too complex, a simpler approach is polling: e.g., poll the status every few seconds via an API. But that is less elegant. Given we want modern design, we lean towards using gRPC streaming or websockets. Possibly we can use **WebTransport** in the future (once gRPC-web supports it) for full bi-di in browser, but for now server-stream is enough.
* We'll incorporate a **retry logic** for real-time streams: if a WebSocket or stream disconnects (network glitch), the frontend automatically attempts to reconnect after a short delay (maybe with exponential backoff, up to a limit). We ensure the UI gracefully handles missing updates during reconnect (maybe by showing “Reconnecting…” if needed).

**Error Handling and Reconciliation:** The frontend should assume that network calls can fail or produce stale data, so it implements reconciliation loops. For example, if a user initiates a download of a new media, the UI immediately adds an item to the list with status “Pending”. It doesn’t wait for the server to confirm. This **optimistic update** improves responsiveness. If the server call fails, we revert that change and show an error message (maybe the item disappears or shows “Failed to add”). We unify error handling so that any gRPC error code from the backend is mapped to a user-friendly message. Possibly maintain a map: NotFound -> “Item not found”, etc.

* We also include global error boundary components in React so if something unforeseen happens (uncaught exception in render), we capture it and show a fallback UI instead of a blank screen.

**Performance & Optimization:** We utilize **code splitting** (via dynamic import or React.lazy) to split the app into chunks (e.g., admin screens separate, heavy components separate). We memoize expensive components or use `React.memo` where appropriate to avoid re-renders on unrelated state changes. State is normalized (especially if using Redux – e.g., store media items in a dictionary by ID to avoid deep prop drilling).

* For lists (like a media gallery), if large, we consider virtualization (using a library like react-window) for efficiency.

**Testing Frontend:** We do unit tests for reducers (Redux logic can be tested by dispatching actions and checking state). We also do component tests with Jest + React Testing Library to ensure UI renders correct for given state. And we do end-to-end tests with a real browser to test the whole flow with a running backend (maybe using a mock backend or our staging environment).

**UI/UX considerations:** The front-end will likely have a video player for streaming HLS. We might use an existing library like hls.js or Video.js to handle HLS playback in browser. That’s decoupled from our state mgmt but integrated when user clicks “Play”.

**Documentation:** We keep the frontend code documented (JSDoc for functions, etc.), and possibly use Storybook to document and visually test components in isolation.

**Integration with Backend Auth:** If using an auth system, the front-end manages tokens (stores JWT in memory or cookie) and attaches them to gRPC-web metadata or HTTP headers for REST. Possibly use an interceptor on the grpc-web client to add auth metadata on each call.

**Handling Offline/Reconnect:** If the app is used in environments with flaky internet, we implement a basic detect of lost connection (maybe via WebSocket status or just catching fetch errors) to alert the user and disable actions until reconnected, then maybe attempt to reload data that might have changed while offline.

In summary, the front-end applies a **modular architecture**: separation of presentational components and stateful logic (often via hooks or container components), use of proven patterns (Redux Toolkit for global state) to manage complexity, and ensures seamless real-time experience via gRPC-web or WebSockets. The design favors readability and maintainability as well – for example, rather than deeply nesting context providers, we structure it so that each feature’s state management is well-encapsulated (the Stackademic article we saw advocates a clean architecture on the front-end as well, with separation of store, use cases, view models, etc.). We may follow a similar approach to keep our front-end codebase organized by feature (each feature might have domain, store, UI subdirectories, etc.).

Finally, we ensure the front-end is **resilient**: it handles partial availability gracefully. E.g., if the Transcoding service is down (so progress updates stall), the UI should timeout and show “Progress unavailable” rather than spin forever. These considerations make the overall user experience robust even when the system is under maintenance or experiencing issues.

## 11. Code Quality & Maintainability

Maintaining high code quality over time is paramount. We establish processes and use tools to enforce best practices, catch bugs early, and ensure the system remains robust and secure.

**Coding Standards & Style:** We adopt a consistent coding style for both Go and TypeScript. For Go, we follow **Effective Go** and idiomatic patterns. We enable linters to enforce formatting (though `go fmt` autoformats anyway), naming conventions, error handling practices, etc. For TypeScript, we use ESLint with recommended rules and perhaps Prettier for formatting. A style guide document is created (or we use industry standard ones) so all contributors write code in a uniform way (for example, how to structure error messages, when to use pointers vs values in Go structs, etc.).

**Linters and Static Analysis:** We integrate **golangci-lint** in our CI pipeline, configured with a broad set of linters:

* **staticcheck** (catches common bugs or suboptimal code),
* **errcheck** (ensures we don’t ignore errors inadvertently),
* **govet** (built-in Go vet),
* **ineffassign** (find unused assignments),
* **goconst** (suggests constants for repeated strings),
* etc.
  This aggregator ensures that if someone introduces something suspicious, CI fails. As noted in a Go security best practices article, *“Go has excellent tooling for static analysis and code quality. Use golangci-lint (which includes staticcheck, errcheck, etc.) and gosec for security scanning.”*. We indeed run **gosec** as part of golangci-lint or separately to catch things like usage of `os.Exec` with unsanitized input, file permission issues, use of `http.Redirect` with user input (Open redirect), and so on.

For the frontend, similarly, we use ESLint and perhaps TypeScript’s strict compiler options to catch undefined behaviors. We treat TypeScript compiler warnings as errors in CI to ensure type safety (enable `strict: true` in tsconfig). We can also use tools like **depcheck** to remove unused dependencies to keep things tidy.

**Security Scanning:** In addition to gosec, we use **govulncheck** (Go’s official tool to scan for known vulnerabilities in dependencies). This will alert us if, say, a version of a library we use has a CVE. We schedule this to run regularly or as part of CI on new dependency introduction. For containers, we incorporate **Trivy** to scan our built Docker images for OS package vulnerabilities and outdated libraries. Trivy can be run in CI; it will produce a report if any high severity issues are found (for example, an outdated OpenSSL in the base image). We make it a policy to keep base images updated frequently to pull in security patches. We also watch for announcements in projects we use (like any critical bug in Envoy, etc., to update promptly).

**Dependency Management:** We pin versions (Go modules with specific versions, npm with lockfile) to ensure builds are reproducible. Renovatebot or Dependabot can be enabled to automatically propose updates to dependencies, which we review and test promptly. This ensures we don’t fall behind on important fixes.

**Peer Code Reviews:** All significant changes undergo code review by at least one other team member. We have guidelines for reviewers to check for: adherence to design, clarity, tests included, proper error handling, no obvious performance issues, etc. We may use a checklist or PR template to remind of these (like “did you run tests? is there an ADR update needed? etc.”).

**Architecture Decision Records (ADRs):** We maintain ADRs as deliverables for major decisions (for example, “Decision: Use NATS JetStream for event bus – Alternatives considered: Kafka, RabbitMQ; Rationale: ...”). Each ADR is stored in version control, so any changes to architecture are documented. This avoids repeated debates and on-boarding new devs is easier because they can read through ADRs to grasp why things are done a certain way. We also log decisions like “Use Istio vs Linkerd” or “Use Google Wire for dependency injection in Go”.

**Documentation:** We auto-generate API documentation from our code. For gRPC, we generate reference docs (perhaps using a tool like `protoc-gen-doc` to output Markdown or HTML from proto comments). These include all RPCs and message fields, which we publish for internal use (maybe on a simple docs site). We also maintain a **README** or a MkDocs site covering how each service is structured, how to run it, etc. Code is liberally commented especially in complex areas. For instance, any non-obvious algorithm or concurrency usage in code is documented with comments so future maintainers understand the intent. We might also generate GoDoc documentation and host it (or just use pkg.go.dev for public ones if open source).

**Automated Documentation:** We annotate our Kubernetes CRDs with descriptions, so `kubectl explain` works. We might generate a CRD reference doc from those. We could also integrate docs generation for OpenAPI (via grpc-gateway annotations) so that there’s a swagger UI for REST endpoints if needed.

**Continuous Integration/Deployment:** We set up CI to run tests and quality checks on every commit. We possibly use a pipeline (GitHub Actions, GitLab CI, etc.) with stages for build, test, scan, etc., as described. Only after passing all checks does code get merged. We also might have protected main branch requiring PRs and reviews, ensuring no one accidentally pushes unreviewed code.

**Performance Benchmarks:** For critical sections (like a function that processes thousands of items or a custom data structure), we add Go benchmark tests (functions starting with `Benchmark...`) to measure performance. This not only helps us optimize but also ensures performance regressions can be caught (some CI setups allow tracking benchmark results over time). If we optimize something (like use sync.Pool), we accompany it with a benchmark to prove it improves things. We might include results in comments or ADRs (like “Switching from JSON to protobuf for internal service X improved throughput by 50%").

**Memory Management:** In Go, one has to be mindful of allocations in hot paths. We use tools like pprof to see if we need object pooling. For example, if our transcoding service enqueues lots of tasks, we ensure we reuse buffers or use sync.Pool for objects that are frequently allocated, to reduce GC pressure. We also avoid large memory footprints by streaming data where possible (e.g., not reading entire video into memory, but streaming from disk to FFmpeg via pipe if needed, though probably FFmpeg reads file itself). We also ensure to limit concurrency to avoid out-of-memory (especially if multiple FFmpeg processes on one node – that's more cluster config though).

**Connection Management:** We ensure not to leak resources: database connections are closed (using connection pool properly, and context cancellation). gRPC client connections are reused and closed on service shutdown. File descriptors are closed after use. Linters and tests help catch some of these (e.g., `resourcecheck` linter ensures file closers are called).

**Pooling and Concurrency:** We use worker pools for things like sending emails or thumbnails generation if needed. For example, if Media service had to generate thumbnails for episodes, we could have a pool of goroutines doing that reading from a channel, to not spawn unlimited goroutines. We avoid global unbounded goroutines to prevent memory leaks; any background goroutine has a defined purpose and lifecycle (tie it to service context so it stops on shutdown). We might use **errgroup** (from `x/sync/errgroup`) for managing multiple concurrent tasks within a request to cancel all on one failure.

**Maintainability of Microservices:** We avoid duplication by factoring common utilities into internal packages or a shared module. For instance, our gRPC interceptors for logging and tracing might live in a `pkg/common/grpcmiddleware` module that all services import. This way, improvements are propagated. But we be careful to not create a monolithic dependency – it should be small and stable or vendored.

**Version Control and Releases:** We version our services (via Git tags or Docker image tags). We maintain a changelog for each service so it’s clear what changed in each version (could be in CHANGELOG.md, updated per release). We consider semantic versioning for APIs (like if a breaking change in proto, that’s a major version bump). However, ideally we avoid breaking changes by using new fields and new methods.

**Refactoring Safety:** With strong tests in place, we are confident to refactor when needed (e.g., change database schema or split a service). The tests and CI act as a safety net. We encourage small refactorings continuously to keep code clean (like if a function is too long or duplicated logic appears, factor it out promptly rather than letting tech debt accumulate).

**Monitoring Code Quality Metrics:** We might use a tool like SonarQube or CodeClimate that statically analyzes code for maintainability (cyclomatic complexity, duplication). For example, if a function has too high complexity, we consider splitting it. If code duplication occurs (like similar block in two services), maybe abstract it to common lib. These tools give a high-level picture (like a maintainability index). We treat those as advisory but useful for long-term health.

**Documentation of Runbooks:** For maintainers and on-call engineers, we write runbooks for common procedures: e.g., “How to restart the system”, “How to scale the transcoder nodes”, “How to recover if a video fails to transcode (steps to re-run)”. This is more ops documentation but part of maintainability as well (ease of operations).

By combining these practices – **automated linting, rigorous testing, code reviews, ADR documentation, and continuous monitoring of code health** – we ensure the codebase remains clean, secure, and efficient over its lifecycle. Every team member is responsible for quality: e.g., if someone finds a potential issue or better pattern, they can propose it (maybe via an ADR or at least an issue). There’s an emphasis on being proactive (updating dependencies, fixing warnings) so that we don’t accrue “software rust.”

Finally, we foster a culture where performance and correctness are both valued. We do post-mortems on any production issue to derive action items (like “add a test for scenario X” or “add a linter to catch Y”) thereby continuously improving quality processes. The end goal is a robust system that can evolve easily and be trusted by its operators and users.

**Sources:**

* Sairyss DDD & Hexagonal Patterns
* gRPC Best Practices (CoreOSFest notes)
* Victoriametrics blog on gRPC (error mapping)
* Red Hat K8s Operators Best Practices
* Synadia NATS vs Kafka comparison
* Temporal Saga Pattern discussion
* Egnyte transcoding at scale
* Istio Circuit Breaker docs
* Logging best practices (BetterStack)
* Go Security/Quality Practices
* Stackademic Clean Architecture React series
